Tactic  ID,Tactic Description,Technique ID,Technique Description,Tactics used in the Attack,Attack Desc Using Tactics,Techniques used in the attack,Attack Desc Using Technique
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0000,"Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.
The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.
Organizations often use open source model architectures trained on additional proprietary data in production.
Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy ML Model](/techniques/AML.T0005)).
An adversary can search these resources for publications for authors employed at the victim organization.

Research materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).
",AML.TA0002,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0000.001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0000,"Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.
The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.
Organizations often use open source model architectures trained on additional proprietary data in production.
Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy ML Model](/techniques/AML.T0005)).
An adversary can search these resources for publications for authors employed at the victim organization.

Research materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).
",AML.TA0002,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0000,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0000,"Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.
The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.
Organizations often use open source model architectures trained on additional proprietary data in production.
Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy ML Model](/techniques/AML.T0005)).
An adversary can search these resources for publications for authors employed at the victim organization.

Research materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).
",AML.TA0002,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0000,"Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.
The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.
Organizations often use open source model architectures trained on additional proprietary data in production.
Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy ML Model](/techniques/AML.T0005)).
An adversary can search these resources for publications for authors employed at the victim organization.

Research materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).
",AML.TA0002,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0000,"Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.
The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.
Organizations often use open source model architectures trained on additional proprietary data in production.
Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy ML Model](/techniques/AML.T0005)).
An adversary can search these resources for publications for authors employed at the victim organization.

Research materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).
",AML.TA0002,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0000,"Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.
The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.
Organizations often use open source model architectures trained on additional proprietary data in production.
Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy ML Model](/techniques/AML.T0005)).
An adversary can search these resources for publications for authors employed at the victim organization.

Research materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).
",AML.TA0002,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0000,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0000,"Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.
The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.
Organizations often use open source model architectures trained on additional proprietary data in production.
Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy ML Model](/techniques/AML.T0005)).
An adversary can search these resources for publications for authors employed at the victim organization.

Research materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).
",AML.TA0002,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0000,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0000,"Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.
The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.
Organizations often use open source model architectures trained on additional proprietary data in production.
Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy ML Model](/techniques/AML.T0005)).
An adversary can search these resources for publications for authors employed at the victim organization.

Research materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).
",AML.TA0002,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0000,"Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.
The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.
Organizations often use open source model architectures trained on additional proprietary data in production.
Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy ML Model](/techniques/AML.T0005)).
An adversary can search these resources for publications for authors employed at the victim organization.

Research materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).
",AML.TA0002,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0004,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0000,"Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.
The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.
Organizations often use open source model architectures trained on additional proprietary data in production.
Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy ML Model](/techniques/AML.T0005)).
An adversary can search these resources for publications for authors employed at the victim organization.

Research materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).
",AML.TA0002,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0000,"Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.
The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.
Organizations often use open source model architectures trained on additional proprietary data in production.
Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy ML Model](/techniques/AML.T0005)).
An adversary can search these resources for publications for authors employed at the victim organization.

Research materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).
",AML.TA0002,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0001,"Much like the [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000), there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.
This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) their own if necessary.",AML.TA0002,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0000.001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0001,"Much like the [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000), there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.
This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) their own if necessary.",AML.TA0002,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0000,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0001,"Much like the [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000), there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.
This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) their own if necessary.",AML.TA0002,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0001,"Much like the [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000), there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.
This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) their own if necessary.",AML.TA0002,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0001,"Much like the [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000), there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.
This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) their own if necessary.",AML.TA0002,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0001,"Much like the [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000), there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.
This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) their own if necessary.",AML.TA0002,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0000,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0001,"Much like the [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000), there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.
This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) their own if necessary.",AML.TA0002,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0000,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0001,"Much like the [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000), there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.
This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) their own if necessary.",AML.TA0002,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0001,"Much like the [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000), there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.
This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) their own if necessary.",AML.TA0002,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0004,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0001,"Much like the [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000), there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.
This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) their own if necessary.",AML.TA0002,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0001,"Much like the [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000), there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.
This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) their own if necessary.",AML.TA0002,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0003,"Adversaries may search websites owned by the victim for information that can be used during targeting.
Victim-owned websites may contain technical details about their ML-enabled products or services.
Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.
These sites may also have details highlighting business operations and relationships.

Adversaries may search victim-owned websites to gather actionable information.
This information may help adversaries tailor their attacks (e.g. [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) or [Manual Modification](/techniques/AML.T0043.003)).
Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000) or [Search for Publicly Available Adversarial Vulnerability Analysis](/techniques/AML.T0001))
",AML.TA0002,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0000.001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0003,"Adversaries may search websites owned by the victim for information that can be used during targeting.
Victim-owned websites may contain technical details about their ML-enabled products or services.
Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.
These sites may also have details highlighting business operations and relationships.

Adversaries may search victim-owned websites to gather actionable information.
This information may help adversaries tailor their attacks (e.g. [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) or [Manual Modification](/techniques/AML.T0043.003)).
Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000) or [Search for Publicly Available Adversarial Vulnerability Analysis](/techniques/AML.T0001))
",AML.TA0002,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0000,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0003,"Adversaries may search websites owned by the victim for information that can be used during targeting.
Victim-owned websites may contain technical details about their ML-enabled products or services.
Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.
These sites may also have details highlighting business operations and relationships.

Adversaries may search victim-owned websites to gather actionable information.
This information may help adversaries tailor their attacks (e.g. [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) or [Manual Modification](/techniques/AML.T0043.003)).
Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000) or [Search for Publicly Available Adversarial Vulnerability Analysis](/techniques/AML.T0001))
",AML.TA0002,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0003,"Adversaries may search websites owned by the victim for information that can be used during targeting.
Victim-owned websites may contain technical details about their ML-enabled products or services.
Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.
These sites may also have details highlighting business operations and relationships.

Adversaries may search victim-owned websites to gather actionable information.
This information may help adversaries tailor their attacks (e.g. [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) or [Manual Modification](/techniques/AML.T0043.003)).
Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000) or [Search for Publicly Available Adversarial Vulnerability Analysis](/techniques/AML.T0001))
",AML.TA0002,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0003,"Adversaries may search websites owned by the victim for information that can be used during targeting.
Victim-owned websites may contain technical details about their ML-enabled products or services.
Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.
These sites may also have details highlighting business operations and relationships.

Adversaries may search victim-owned websites to gather actionable information.
This information may help adversaries tailor their attacks (e.g. [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) or [Manual Modification](/techniques/AML.T0043.003)).
Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000) or [Search for Publicly Available Adversarial Vulnerability Analysis](/techniques/AML.T0001))
",AML.TA0002,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0003,"Adversaries may search websites owned by the victim for information that can be used during targeting.
Victim-owned websites may contain technical details about their ML-enabled products or services.
Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.
These sites may also have details highlighting business operations and relationships.

Adversaries may search victim-owned websites to gather actionable information.
This information may help adversaries tailor their attacks (e.g. [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) or [Manual Modification](/techniques/AML.T0043.003)).
Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000) or [Search for Publicly Available Adversarial Vulnerability Analysis](/techniques/AML.T0001))
",AML.TA0002,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0000,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0003,"Adversaries may search websites owned by the victim for information that can be used during targeting.
Victim-owned websites may contain technical details about their ML-enabled products or services.
Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.
These sites may also have details highlighting business operations and relationships.

Adversaries may search victim-owned websites to gather actionable information.
This information may help adversaries tailor their attacks (e.g. [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) or [Manual Modification](/techniques/AML.T0043.003)).
Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000) or [Search for Publicly Available Adversarial Vulnerability Analysis](/techniques/AML.T0001))
",AML.TA0002,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0000,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0003,"Adversaries may search websites owned by the victim for information that can be used during targeting.
Victim-owned websites may contain technical details about their ML-enabled products or services.
Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.
These sites may also have details highlighting business operations and relationships.

Adversaries may search victim-owned websites to gather actionable information.
This information may help adversaries tailor their attacks (e.g. [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) or [Manual Modification](/techniques/AML.T0043.003)).
Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000) or [Search for Publicly Available Adversarial Vulnerability Analysis](/techniques/AML.T0001))
",AML.TA0002,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0003,"Adversaries may search websites owned by the victim for information that can be used during targeting.
Victim-owned websites may contain technical details about their ML-enabled products or services.
Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.
These sites may also have details highlighting business operations and relationships.

Adversaries may search victim-owned websites to gather actionable information.
This information may help adversaries tailor their attacks (e.g. [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) or [Manual Modification](/techniques/AML.T0043.003)).
Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000) or [Search for Publicly Available Adversarial Vulnerability Analysis](/techniques/AML.T0001))
",AML.TA0002,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0004,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0003,"Adversaries may search websites owned by the victim for information that can be used during targeting.
Victim-owned websites may contain technical details about their ML-enabled products or services.
Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.
These sites may also have details highlighting business operations and relationships.

Adversaries may search victim-owned websites to gather actionable information.
This information may help adversaries tailor their attacks (e.g. [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) or [Manual Modification](/techniques/AML.T0043.003)).
Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000) or [Search for Publicly Available Adversarial Vulnerability Analysis](/techniques/AML.T0001))
",AML.TA0002,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0003,"Adversaries may search websites owned by the victim for information that can be used during targeting.
Victim-owned websites may contain technical details about their ML-enabled products or services.
Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.
These sites may also have details highlighting business operations and relationships.

Adversaries may search victim-owned websites to gather actionable information.
This information may help adversaries tailor their attacks (e.g. [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) or [Manual Modification](/techniques/AML.T0043.003)).
Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000) or [Search for Publicly Available Adversarial Vulnerability Analysis](/techniques/AML.T0001))
",AML.TA0002,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0004,"Adversaries may search open application repositories during targeting.
Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.

Adversaries may craft search queries seeking applications that contain a ML-enabled components.
Frequently, the next step is to [Acquire Public ML Artifacts](/techniques/AML.T0002).
",AML.TA0002,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0000.001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0004,"Adversaries may search open application repositories during targeting.
Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.

Adversaries may craft search queries seeking applications that contain a ML-enabled components.
Frequently, the next step is to [Acquire Public ML Artifacts](/techniques/AML.T0002).
",AML.TA0002,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0000,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0004,"Adversaries may search open application repositories during targeting.
Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.

Adversaries may craft search queries seeking applications that contain a ML-enabled components.
Frequently, the next step is to [Acquire Public ML Artifacts](/techniques/AML.T0002).
",AML.TA0002,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0004,"Adversaries may search open application repositories during targeting.
Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.

Adversaries may craft search queries seeking applications that contain a ML-enabled components.
Frequently, the next step is to [Acquire Public ML Artifacts](/techniques/AML.T0002).
",AML.TA0002,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0004,"Adversaries may search open application repositories during targeting.
Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.

Adversaries may craft search queries seeking applications that contain a ML-enabled components.
Frequently, the next step is to [Acquire Public ML Artifacts](/techniques/AML.T0002).
",AML.TA0002,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0004,"Adversaries may search open application repositories during targeting.
Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.

Adversaries may craft search queries seeking applications that contain a ML-enabled components.
Frequently, the next step is to [Acquire Public ML Artifacts](/techniques/AML.T0002).
",AML.TA0002,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0000,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0004,"Adversaries may search open application repositories during targeting.
Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.

Adversaries may craft search queries seeking applications that contain a ML-enabled components.
Frequently, the next step is to [Acquire Public ML Artifacts](/techniques/AML.T0002).
",AML.TA0002,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0000,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0004,"Adversaries may search open application repositories during targeting.
Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.

Adversaries may craft search queries seeking applications that contain a ML-enabled components.
Frequently, the next step is to [Acquire Public ML Artifacts](/techniques/AML.T0002).
",AML.TA0002,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0004,"Adversaries may search open application repositories during targeting.
Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.

Adversaries may craft search queries seeking applications that contain a ML-enabled components.
Frequently, the next step is to [Acquire Public ML Artifacts](/techniques/AML.T0002).
",AML.TA0002,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0004,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0004,"Adversaries may search open application repositories during targeting.
Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.

Adversaries may craft search queries seeking applications that contain a ML-enabled components.
Frequently, the next step is to [Acquire Public ML Artifacts](/techniques/AML.T0002).
",AML.TA0002,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0004,"Adversaries may search open application repositories during targeting.
Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.

Adversaries may craft search queries seeking applications that contain a ML-enabled components.
Frequently, the next step is to [Acquire Public ML Artifacts](/techniques/AML.T0002).
",AML.TA0002,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0006,"An adversary may probe or scan the victim system to gather information for targeting.
This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.
",AML.TA0002,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0000.001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0006,"An adversary may probe or scan the victim system to gather information for targeting.
This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.
",AML.TA0002,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0000,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0006,"An adversary may probe or scan the victim system to gather information for targeting.
This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.
",AML.TA0002,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0006,"An adversary may probe or scan the victim system to gather information for targeting.
This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.
",AML.TA0002,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0006,"An adversary may probe or scan the victim system to gather information for targeting.
This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.
",AML.TA0002,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0006,"An adversary may probe or scan the victim system to gather information for targeting.
This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.
",AML.TA0002,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0000,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0006,"An adversary may probe or scan the victim system to gather information for targeting.
This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.
",AML.TA0002,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0000,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0006,"An adversary may probe or scan the victim system to gather information for targeting.
This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.
",AML.TA0002,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0006,"An adversary may probe or scan the victim system to gather information for targeting.
This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.
",AML.TA0002,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0004,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0006,"An adversary may probe or scan the victim system to gather information for targeting.
This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.
",AML.TA0002,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0002,"The adversary is trying to gather information they can use to plan
future operations.

Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.
Such information may include details of the victim organizations machine learning capabilities and research efforts.
This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.
",AML.T0006,"An adversary may probe or scan the victim system to gather information for targeting.
This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.
",AML.TA0002,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0002.000,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0002,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0017,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
",AML.T0016.000,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0017,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0008.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.000,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0021,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.001,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0008.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002.000,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0002,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0002.000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0002.001,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0017,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0002.000,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0002,"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.
These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.
An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.
Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).
These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).
If these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).
Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.
",AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0017,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0002.000,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0002,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0017,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
",AML.T0016.000,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0017,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0008.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.000,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0021,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.001,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0008.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002.000,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0002,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0002.000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0002.001,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0017,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0002.000,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0016,"Adversaries may search for and obtain software capabilities for use in their operations.
Capabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0017,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0002.000,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0002,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0017,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
",AML.T0016.000,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0017,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0008.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.000,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0021,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.001,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0008.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002.000,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0002,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0002.000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0002.001,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0017,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0002.000,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0017,Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.,AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0017,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0002.000,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0002,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0017,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
",AML.T0016.000,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0017,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0008.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.000,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0021,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.001,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0008.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002.000,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0002,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0002.000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0002.001,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0017,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0002.000,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0008,"Adversaries may buy, lease, or rent infrastructure for use throughout their operation.
A wide variety of infrastructure exists for hosting and orchestrating adversary operations.
Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.
Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.
Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.
Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.
",AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0017,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0002.000,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0002,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0017,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
",AML.T0016.000,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0017,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0008.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.000,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0021,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.001,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0008.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002.000,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0002,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0002.000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0002.001,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0017,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0002.000,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0019,"Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.
The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.
This data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).
",AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0017,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0004,"The adversary is trying to gain access to the system containing machine learning artifacts.

The target system could be a network, mobile device, or an edge device such as a sensor platform.
The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities.

Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.
",AML.T0010,"Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.
This could include [GPU Hardware](/techniques/AML.T0010.000), [Data](/techniques/AML.T0010.002) and its annotations, parts of the ML [ML Software](/techniques/AML.T0010.001) stack, or the [Model](/techniques/AML.T0010.003) itself.
In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.
",AML.TA0004,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
",AML.T0010.002,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
"
AML.TA0004,"The adversary is trying to gain access to the system containing machine learning artifacts.

The target system could be a network, mobile device, or an edge device such as a sensor platform.
The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities.

Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.
",AML.T0010,"Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.
This could include [GPU Hardware](/techniques/AML.T0010.000), [Data](/techniques/AML.T0010.002) and its annotations, parts of the ML [ML Software](/techniques/AML.T0010.001) stack, or the [Model](/techniques/AML.T0010.003) itself.
In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.
",AML.TA0004,"Clearview AI's source code repository, though password protected, was misconfigured to allow an arbitrary user to register an account.
This allowed an external researcher to gain access to a private code repository that contained Clearview AI production credentials, keys to cloud storage buckets containing 70K video samples, and copies of its applications and Slack tokens.
With access to training data, a bad-actor has the ability to cause an arbitrary misclassification in the deployed model.
These kinds of attacks illustrate that any attempt to secure ML system should be on top of ""traditional"" good cybersecurity hygiene such as locking down the system with least privileges, multi-factor authentication and monitoring and auditing.
",AML.T0012,"Clearview AI's source code repository, though password protected, was misconfigured to allow an arbitrary user to register an account.
This allowed an external researcher to gain access to a private code repository that contained Clearview AI production credentials, keys to cloud storage buckets containing 70K video samples, and copies of its applications and Slack tokens.
With access to training data, a bad-actor has the ability to cause an arbitrary misclassification in the deployed model.
These kinds of attacks illustrate that any attempt to secure ML system should be on top of ""traditional"" good cybersecurity hygiene such as locking down the system with least privileges, multi-factor authentication and monitoring and auditing.
"
AML.TA0004,"The adversary is trying to gain access to the system containing machine learning artifacts.

The target system could be a network, mobile device, or an edge device such as a sensor platform.
The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities.

Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.
",AML.T0010,"Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.
This could include [GPU Hardware](/techniques/AML.T0010.000), [Data](/techniques/AML.T0010.002) and its annotations, parts of the ML [ML Software](/techniques/AML.T0010.001) stack, or the [Model](/techniques/AML.T0010.003) itself.
In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.
",AML.TA0004,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
",AML.T0010.002,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
"
AML.TA0004,"The adversary is trying to gain access to the system containing machine learning artifacts.

The target system could be a network, mobile device, or an edge device such as a sensor platform.
The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities.

Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.
",AML.T0010,"Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.
This could include [GPU Hardware](/techniques/AML.T0010.000), [Data](/techniques/AML.T0010.002) and its annotations, parts of the ML [ML Software](/techniques/AML.T0010.001) stack, or the [Model](/techniques/AML.T0010.003) itself.
In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.
",AML.TA0004,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0012,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0004,"The adversary is trying to gain access to the system containing machine learning artifacts.

The target system could be a network, mobile device, or an edge device such as a sensor platform.
The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities.

Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.
",AML.T0010,"Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.
This could include [GPU Hardware](/techniques/AML.T0010.000), [Data](/techniques/AML.T0010.002) and its annotations, parts of the ML [ML Software](/techniques/AML.T0010.001) stack, or the [Model](/techniques/AML.T0010.003) itself.
In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.
",AML.TA0004,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0012,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0004,"The adversary is trying to gain access to the system containing machine learning artifacts.

The target system could be a network, mobile device, or an edge device such as a sensor platform.
The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities.

Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.
",AML.T0010,"Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.
This could include [GPU Hardware](/techniques/AML.T0010.000), [Data](/techniques/AML.T0010.002) and its annotations, parts of the ML [ML Software](/techniques/AML.T0010.001) stack, or the [Model](/techniques/AML.T0010.003) itself.
In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.
",AML.TA0004,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0010.003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0040,"Adversaries may gain access to a model via legitimate access to the inference API.
Inference API access can be a source of information to the adversary ([Discover ML Model Ontology](/techniques/AML.T0013), [Discover ML Model Family](/techniques/AML.T0013)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade ML Model](/techniques/AML.T0015), [Erode ML Model Integrity](/techniques/AML.T0031)).
",AML.TA0000,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0047,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0040,"Adversaries may gain access to a model via legitimate access to the inference API.
Inference API access can be a source of information to the adversary ([Discover ML Model Ontology](/techniques/AML.T0013), [Discover ML Model Family](/techniques/AML.T0013)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade ML Model](/techniques/AML.T0015), [Erode ML Model Integrity](/techniques/AML.T0031)).
",AML.TA0000,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0047,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0040,"Adversaries may gain access to a model via legitimate access to the inference API.
Inference API access can be a source of information to the adversary ([Discover ML Model Ontology](/techniques/AML.T0013), [Discover ML Model Family](/techniques/AML.T0013)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade ML Model](/techniques/AML.T0015), [Erode ML Model Integrity](/techniques/AML.T0031)).
",AML.TA0000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0040,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0040,"Adversaries may gain access to a model via legitimate access to the inference API.
Inference API access can be a source of information to the adversary ([Discover ML Model Ontology](/techniques/AML.T0013), [Discover ML Model Family](/techniques/AML.T0013)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade ML Model](/techniques/AML.T0015), [Erode ML Model Integrity](/techniques/AML.T0031)).
",AML.TA0000,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
",AML.T0040,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0040,"Adversaries may gain access to a model via legitimate access to the inference API.
Inference API access can be a source of information to the adversary ([Discover ML Model Ontology](/techniques/AML.T0013), [Discover ML Model Family](/techniques/AML.T0013)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade ML Model](/techniques/AML.T0015), [Erode ML Model Integrity](/techniques/AML.T0031)).
",AML.TA0000,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0040,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0040,"Adversaries may gain access to a model via legitimate access to the inference API.
Inference API access can be a source of information to the adversary ([Discover ML Model Ontology](/techniques/AML.T0013), [Discover ML Model Family](/techniques/AML.T0013)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade ML Model](/techniques/AML.T0015), [Erode ML Model Integrity](/techniques/AML.T0031)).
",AML.TA0000,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0040,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0040,"Adversaries may gain access to a model via legitimate access to the inference API.
Inference API access can be a source of information to the adversary ([Discover ML Model Ontology](/techniques/AML.T0013), [Discover ML Model Family](/techniques/AML.T0013)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade ML Model](/techniques/AML.T0015), [Erode ML Model Integrity](/techniques/AML.T0031)).
",AML.TA0000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0040,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0040,"Adversaries may gain access to a model via legitimate access to the inference API.
Inference API access can be a source of information to the adversary ([Discover ML Model Ontology](/techniques/AML.T0013), [Discover ML Model Family](/techniques/AML.T0013)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade ML Model](/techniques/AML.T0015), [Erode ML Model Integrity](/techniques/AML.T0031)).
",AML.TA0000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0041,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0040,"Adversaries may gain access to a model via legitimate access to the inference API.
Inference API access can be a source of information to the adversary ([Discover ML Model Ontology](/techniques/AML.T0013), [Discover ML Model Family](/techniques/AML.T0013)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade ML Model](/techniques/AML.T0015), [Erode ML Model Integrity](/techniques/AML.T0031)).
",AML.TA0000,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0044,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0040,"Adversaries may gain access to a model via legitimate access to the inference API.
Inference API access can be a source of information to the adversary ([Discover ML Model Ontology](/techniques/AML.T0013), [Discover ML Model Family](/techniques/AML.T0013)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade ML Model](/techniques/AML.T0015), [Erode ML Model Integrity](/techniques/AML.T0031)).
",AML.TA0000,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0041,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0040,"Adversaries may gain access to a model via legitimate access to the inference API.
Inference API access can be a source of information to the adversary ([Discover ML Model Ontology](/techniques/AML.T0013), [Discover ML Model Family](/techniques/AML.T0013)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade ML Model](/techniques/AML.T0015), [Erode ML Model Integrity](/techniques/AML.T0031)).
",AML.TA0000,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0047,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0047,"Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model.
This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.
",AML.TA0000,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0047,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0047,"Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model.
This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.
",AML.TA0000,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0047,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0047,"Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model.
This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.
",AML.TA0000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0040,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0047,"Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model.
This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.
",AML.TA0000,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
",AML.T0040,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0047,"Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model.
This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.
",AML.TA0000,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0040,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0047,"Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model.
This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.
",AML.TA0000,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0040,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0047,"Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model.
This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.
",AML.TA0000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0040,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0047,"Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model.
This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.
",AML.TA0000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0041,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0047,"Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model.
This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.
",AML.TA0000,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0044,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0047,"Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model.
This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.
",AML.TA0000,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0041,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0047,"Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model.
This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.
",AML.TA0000,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0047,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0041,"In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.
If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.
By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.
",AML.TA0000,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0047,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0041,"In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.
If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.
By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.
",AML.TA0000,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0047,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0041,"In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.
If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.
By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.
",AML.TA0000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0040,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0041,"In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.
If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.
By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.
",AML.TA0000,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
",AML.T0040,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0041,"In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.
If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.
By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.
",AML.TA0000,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0040,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0041,"In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.
If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.
By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.
",AML.TA0000,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0040,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0041,"In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.
If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.
By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.
",AML.TA0000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0040,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0041,"In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.
If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.
By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.
",AML.TA0000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0041,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0041,"In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.
If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.
By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.
",AML.TA0000,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0044,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0041,"In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.
If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.
By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.
",AML.TA0000,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0041,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0041,"In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.
If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.
By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.
",AML.TA0000,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0047,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0044,"Adversaries may gain full ""white-box"" access to a machine learning model.
This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.
They may exfiltrate the model to [Craft Adversarial Data](/techniques/AML.T0043) and [Verify Attack](/techniques/AML.T0042) in an offline where it is hard to detect their behavior.
",AML.TA0000,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0047,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0044,"Adversaries may gain full ""white-box"" access to a machine learning model.
This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.
They may exfiltrate the model to [Craft Adversarial Data](/techniques/AML.T0043) and [Verify Attack](/techniques/AML.T0042) in an offline where it is hard to detect their behavior.
",AML.TA0000,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0047,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0044,"Adversaries may gain full ""white-box"" access to a machine learning model.
This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.
They may exfiltrate the model to [Craft Adversarial Data](/techniques/AML.T0043) and [Verify Attack](/techniques/AML.T0042) in an offline where it is hard to detect their behavior.
",AML.TA0000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0040,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0044,"Adversaries may gain full ""white-box"" access to a machine learning model.
This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.
They may exfiltrate the model to [Craft Adversarial Data](/techniques/AML.T0043) and [Verify Attack](/techniques/AML.T0042) in an offline where it is hard to detect their behavior.
",AML.TA0000,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
",AML.T0040,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0044,"Adversaries may gain full ""white-box"" access to a machine learning model.
This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.
They may exfiltrate the model to [Craft Adversarial Data](/techniques/AML.T0043) and [Verify Attack](/techniques/AML.T0042) in an offline where it is hard to detect their behavior.
",AML.TA0000,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0040,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0044,"Adversaries may gain full ""white-box"" access to a machine learning model.
This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.
They may exfiltrate the model to [Craft Adversarial Data](/techniques/AML.T0043) and [Verify Attack](/techniques/AML.T0042) in an offline where it is hard to detect their behavior.
",AML.TA0000,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0040,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0044,"Adversaries may gain full ""white-box"" access to a machine learning model.
This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.
They may exfiltrate the model to [Craft Adversarial Data](/techniques/AML.T0043) and [Verify Attack](/techniques/AML.T0042) in an offline where it is hard to detect their behavior.
",AML.TA0000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0040,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0044,"Adversaries may gain full ""white-box"" access to a machine learning model.
This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.
They may exfiltrate the model to [Craft Adversarial Data](/techniques/AML.T0043) and [Verify Attack](/techniques/AML.T0042) in an offline where it is hard to detect their behavior.
",AML.TA0000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0041,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0044,"Adversaries may gain full ""white-box"" access to a machine learning model.
This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.
They may exfiltrate the model to [Craft Adversarial Data](/techniques/AML.T0043) and [Verify Attack](/techniques/AML.T0042) in an offline where it is hard to detect their behavior.
",AML.TA0000,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0044,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0044,"Adversaries may gain full ""white-box"" access to a machine learning model.
This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.
They may exfiltrate the model to [Craft Adversarial Data](/techniques/AML.T0043) and [Verify Attack](/techniques/AML.T0042) in an offline where it is hard to detect their behavior.
",AML.TA0000,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0041,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0000,"An adversary is attempting to gain some level of access to a machine learning model.

ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.
The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.
The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.
",AML.T0044,"Adversaries may gain full ""white-box"" access to a machine learning model.
This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.
They may exfiltrate the model to [Craft Adversarial Data](/techniques/AML.T0043) and [Verify Attack](/techniques/AML.T0042) in an offline where it is hard to detect their behavior.
",AML.TA0000,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0047,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0008,"The adversary is trying to figure out your environment.

Discovery consists of techniques an adversary may use to gain knowledge about the system and internal network.
These techniques help adversaries observe the environment and orient themselves before deciding how to act.
They also allow adversaries to explore what they can control and what's around their entry point in order to discover how it could benefit their current objective.
Native operating system tools are often used toward this post-compromise information-gathering objective.
",AML.T0013,"Adversaries may discover the ontology of a machine learning model's output space, for example, the types of objects a model can detect.
The adversary may discovery the ontology by repeated queries to the model, forcing it to enumerate its output space.
Or the ontology may be discovered in a configuration file or in documentation about the model.

The model ontology helps the adversary understand how the model is being used by the victim.
It is useful to the adversary in creating targeted attacks.
",AML.TA0008,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0013,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0008,"The adversary is trying to figure out your environment.

Discovery consists of techniques an adversary may use to gain knowledge about the system and internal network.
These techniques help adversaries observe the environment and orient themselves before deciding how to act.
They also allow adversaries to explore what they can control and what's around their entry point in order to discover how it could benefit their current objective.
Native operating system tools are often used toward this post-compromise information-gathering objective.
",AML.T0014,"Adversaries may discover the general family of model.
General information about the model may be revealed in documentation, or the adversary may used carefully constructed examples and analyze the model's responses to categorize it.

Knowledge of the model family can help the adversary identify means of attacking the model and help tailor the attack.
",AML.TA0008,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0013,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0002.000,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0002,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0017,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
",AML.T0016.000,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0017,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0008.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.000,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0021,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.001,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0008.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002.000,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0002,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0002.000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0002.001,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0017,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0002.000,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0020,"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.
This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.
Data poisoning attacks may or may not require modifying the labels.
The embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)

Poisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.
",AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0017,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0002.000,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0002,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0017,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
",AML.T0016.000,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0017,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0008.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.001,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0016.000,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0021,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.000,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0002.001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.001,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0002.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0008.000,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0002.000,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0002,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0002.000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0002.001,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0017,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0002.000,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0003,"The adversary is trying to establish resources they can use to support operations.

Resource Development consists of techniques that involve adversaries creating,
purchasing, or compromising/stealing resources that can be used to support targeting.
Such resources include machine learning artifacts, infrastructure, accounts, or capabilities.
These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.
",AML.T0021,"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.
",AML.TA0003,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0017,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0005,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0043.003,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0042,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0043.001,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0042,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
",AML.T0043,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0043.003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0005.001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0043.002,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0005,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0005,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0043.000,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0043.002,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0043.000,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0043.001,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0005,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0043.000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0042,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0043.004,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0005,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0043.002,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0005,"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.
Proxy models are used to simulate complete access to the target model in a fully offline manner.

Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.
",AML.TA0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0042,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0008,"The adversary is trying to figure out your environment.

Discovery consists of techniques an adversary may use to gain knowledge about the system and internal network.
These techniques help adversaries observe the environment and orient themselves before deciding how to act.
They also allow adversaries to explore what they can control and what's around their entry point in order to discover how it could benefit their current objective.
Native operating system tools are often used toward this post-compromise information-gathering objective.
",AML.T0007,"Adversaries may search private sources to identify machine learning artifacts that exist on the system and gather information about them.
These artifacts can include the software stack used to train and deploy models, training and testing data management systems, container registries, software repositories, and model zoos.

This information can be used to identify targets for further collection, exfiltration, or disruption, and to tailor and improve attacks.
",AML.TA0008,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0013,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0004,"The adversary is trying to gain access to the system containing machine learning artifacts.

The target system could be a network, mobile device, or an edge device such as a sensor platform.
The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities.

Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.
",AML.T0012,"Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access.
Credentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services.

Compromised credentials may provide access to additional ML artifacts and allow the adversary to perform [Discover ML Artifacts](/techniques/AML.T0007).
Compromised credentials may also grant and adversary increased privileges such as write access to ML artifacts used during development or production.
",AML.TA0004,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
",AML.T0010.002,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
"
AML.TA0004,"The adversary is trying to gain access to the system containing machine learning artifacts.

The target system could be a network, mobile device, or an edge device such as a sensor platform.
The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities.

Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.
",AML.T0012,"Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access.
Credentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services.

Compromised credentials may provide access to additional ML artifacts and allow the adversary to perform [Discover ML Artifacts](/techniques/AML.T0007).
Compromised credentials may also grant and adversary increased privileges such as write access to ML artifacts used during development or production.
",AML.TA0004,"Clearview AI's source code repository, though password protected, was misconfigured to allow an arbitrary user to register an account.
This allowed an external researcher to gain access to a private code repository that contained Clearview AI production credentials, keys to cloud storage buckets containing 70K video samples, and copies of its applications and Slack tokens.
With access to training data, a bad-actor has the ability to cause an arbitrary misclassification in the deployed model.
These kinds of attacks illustrate that any attempt to secure ML system should be on top of ""traditional"" good cybersecurity hygiene such as locking down the system with least privileges, multi-factor authentication and monitoring and auditing.
",AML.T0012,"Clearview AI's source code repository, though password protected, was misconfigured to allow an arbitrary user to register an account.
This allowed an external researcher to gain access to a private code repository that contained Clearview AI production credentials, keys to cloud storage buckets containing 70K video samples, and copies of its applications and Slack tokens.
With access to training data, a bad-actor has the ability to cause an arbitrary misclassification in the deployed model.
These kinds of attacks illustrate that any attempt to secure ML system should be on top of ""traditional"" good cybersecurity hygiene such as locking down the system with least privileges, multi-factor authentication and monitoring and auditing.
"
AML.TA0004,"The adversary is trying to gain access to the system containing machine learning artifacts.

The target system could be a network, mobile device, or an edge device such as a sensor platform.
The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities.

Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.
",AML.T0012,"Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access.
Credentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services.

Compromised credentials may provide access to additional ML artifacts and allow the adversary to perform [Discover ML Artifacts](/techniques/AML.T0007).
Compromised credentials may also grant and adversary increased privileges such as write access to ML artifacts used during development or production.
",AML.TA0004,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
",AML.T0010.002,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
"
AML.TA0004,"The adversary is trying to gain access to the system containing machine learning artifacts.

The target system could be a network, mobile device, or an edge device such as a sensor platform.
The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities.

Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.
",AML.T0012,"Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access.
Credentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services.

Compromised credentials may provide access to additional ML artifacts and allow the adversary to perform [Discover ML Artifacts](/techniques/AML.T0007).
Compromised credentials may also grant and adversary increased privileges such as write access to ML artifacts used during development or production.
",AML.TA0004,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0012,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0004,"The adversary is trying to gain access to the system containing machine learning artifacts.

The target system could be a network, mobile device, or an edge device such as a sensor platform.
The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities.

Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.
",AML.T0012,"Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access.
Credentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services.

Compromised credentials may provide access to additional ML artifacts and allow the adversary to perform [Discover ML Artifacts](/techniques/AML.T0007).
Compromised credentials may also grant and adversary increased privileges such as write access to ML artifacts used during development or production.
",AML.TA0004,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0012,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0004,"The adversary is trying to gain access to the system containing machine learning artifacts.

The target system could be a network, mobile device, or an edge device such as a sensor platform.
The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities.

Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.
",AML.T0012,"Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access.
Credentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services.

Compromised credentials may provide access to additional ML artifacts and allow the adversary to perform [Discover ML Artifacts](/techniques/AML.T0007).
Compromised credentials may also grant and adversary increased privileges such as write access to ML artifacts used during development or production.
",AML.TA0004,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0010.003,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0007,"The adversary is trying to avoid being detected by security software.

Defense Evasion consists of techniques that adversaries use to avoid detection throughout their compromise.
Techniques used for defense evasion include evading ML-enabled security software such as malware detectors.
",AML.T0015,"Adversaries can [Craft Adversarial Data](/techniques/AML.T0043) that prevent a machine learning model from correctly identifying the contents of the data.
This technique can be used to evade a downstream task where machine learning is utilized.
The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.
",AML.TA0007,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0015,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0007,"The adversary is trying to avoid being detected by security software.

Defense Evasion consists of techniques that adversaries use to avoid detection throughout their compromise.
Techniques used for defense evasion include evading ML-enabled security software such as malware detectors.
",AML.T0015,"Adversaries can [Craft Adversarial Data](/techniques/AML.T0043) that prevent a machine learning model from correctly identifying the contents of the data.
This technique can be used to evade a downstream task where machine learning is utilized.
The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.
",AML.TA0007,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0015,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0007,"The adversary is trying to avoid being detected by security software.

Defense Evasion consists of techniques that adversaries use to avoid detection throughout their compromise.
Techniques used for defense evasion include evading ML-enabled security software such as malware detectors.
",AML.T0015,"Adversaries can [Craft Adversarial Data](/techniques/AML.T0043) that prevent a machine learning model from correctly identifying the contents of the data.
This technique can be used to evade a downstream task where machine learning is utilized.
The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.
",AML.TA0007,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0015,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0007,"The adversary is trying to avoid being detected by security software.

Defense Evasion consists of techniques that adversaries use to avoid detection throughout their compromise.
Techniques used for defense evasion include evading ML-enabled security software such as malware detectors.
",AML.T0015,"Adversaries can [Craft Adversarial Data](/techniques/AML.T0043) that prevent a machine learning model from correctly identifying the contents of the data.
This technique can be used to evade a downstream task where machine learning is utilized.
The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.
",AML.TA0007,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0015,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0006,"The adversary is trying to maintain their foothold.

Persistence consists of techniques that adversaries use to keep access to systems across restarts, changed credentials, and other interruptions that could cut off their access.
Techniques used for persistence often involve leaving behind modified ML artifacts such as poisoned training data or backdoored ML models.
",AML.T0018,"Adversaries may introduce a backdoor into a ML model.
A backdoored model operates performs as expected under typical conditions, but will produce the adversary's desired output when a trigger is introduced to the input data.
A backdoored model provides the adversary with a persistent artifact on the victim system.
The embedded vulnerability is typically activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)
",AML.TA0006,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
",AML.T0020,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
"
AML.TA0006,"The adversary is trying to maintain their foothold.

Persistence consists of techniques that adversaries use to keep access to systems across restarts, changed credentials, and other interruptions that could cut off their access.
Techniques used for persistence often involve leaving behind modified ML artifacts such as poisoned training data or backdoored ML models.
",AML.T0018,"Adversaries may introduce a backdoor into a ML model.
A backdoored model operates performs as expected under typical conditions, but will produce the adversary's desired output when a trigger is introduced to the input data.
A backdoored model provides the adversary with a persistent artifact on the victim system.
The embedded vulnerability is typically activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)
",AML.TA0006,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
",AML.T0020,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
"
AML.TA0006,"The adversary is trying to maintain their foothold.

Persistence consists of techniques that adversaries use to keep access to systems across restarts, changed credentials, and other interruptions that could cut off their access.
Techniques used for persistence often involve leaving behind modified ML artifacts such as poisoned training data or backdoored ML models.
",AML.T0018,"Adversaries may introduce a backdoor into a ML model.
A backdoored model operates performs as expected under typical conditions, but will produce the adversary's desired output when a trigger is introduced to the input data.
A backdoored model provides the adversary with a persistent artifact on the victim system.
The embedded vulnerability is typically activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)
",AML.TA0006,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0018.000,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0029,"Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service.
Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded.
Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system.
",AML.TA0011,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0015,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0029,"Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service.
Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded.
Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system.
",AML.TA0011,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0045,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0029,"Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service.
Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded.
Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system.
",AML.TA0011,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0015,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0029,"Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service.
Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded.
Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system.
",AML.TA0011,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
",AML.T0031,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0029,"Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service.
Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded.
Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system.
",AML.TA0011,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0015,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0029,"Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service.
Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded.
Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system.
",AML.TA0011,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0015,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0029,"Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service.
Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded.
Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system.
",AML.TA0011,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0015,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0029,"Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service.
Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded.
Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system.
",AML.TA0011,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0015,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0046,"Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections.
This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.
",AML.TA0011,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0015,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0046,"Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections.
This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.
",AML.TA0011,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0045,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0046,"Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections.
This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.
",AML.TA0011,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0015,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0046,"Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections.
This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.
",AML.TA0011,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
",AML.T0031,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0046,"Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections.
This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.
",AML.TA0011,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0015,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0046,"Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections.
This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.
",AML.TA0011,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0015,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0046,"Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections.
This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.
",AML.TA0011,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0015,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0046,"Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections.
This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.
",AML.TA0011,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0015,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0031,"Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.
This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.
",AML.TA0011,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0015,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0031,"Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.
This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.
",AML.TA0011,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0045,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0031,"Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.
This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.
",AML.TA0011,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0015,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0031,"Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.
This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.
",AML.TA0011,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
",AML.T0031,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0031,"Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.
This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.
",AML.TA0011,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0015,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0031,"Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.
This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.
",AML.TA0011,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0015,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0031,"Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.
This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.
",AML.TA0011,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0015,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0031,"Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.
This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.
",AML.TA0011,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0015,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0034,"Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization.
Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.
",AML.TA0011,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0015,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0034,"Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization.
Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.
",AML.TA0011,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0045,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0034,"Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization.
Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.
",AML.TA0011,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0015,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0034,"Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization.
Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.
",AML.TA0011,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
",AML.T0031,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0034,"Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization.
Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.
",AML.TA0011,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0015,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0034,"Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization.
Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.
",AML.TA0011,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0015,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0034,"Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization.
Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.
",AML.TA0011,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0015,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0034,"Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization.
Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.
",AML.TA0011,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0015,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0009,"The adversary is trying to gather ML artifacts and other related information relevant to their goal.

Collection consists of techniques adversaries may use to gather information and the sources information is collected from that are relevant to following through on the adversary's objectives.
Frequently, the next goal after collecting data is to steal (exfiltrate) the ML artifacts, or use the collected information to stage future operations.
Common target sources include software repositories, container registries, model repositories, and object stores.
",AML.T0035,"Adversaries may collect ML artifacts for [Exfiltration](/tactics/AML.TA0010) or for use in [ML Attack Staging](/tactics/AML.TA0001).
ML artifacts include models and datasets as well as other telemetry data produced when interacting with a model.
",AML.TA0009,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0036,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0009,"The adversary is trying to gather ML artifacts and other related information relevant to their goal.

Collection consists of techniques adversaries may use to gather information and the sources information is collected from that are relevant to following through on the adversary's objectives.
Frequently, the next goal after collecting data is to steal (exfiltrate) the ML artifacts, or use the collected information to stage future operations.
Common target sources include software repositories, container registries, model repositories, and object stores.
",AML.T0035,"Adversaries may collect ML artifacts for [Exfiltration](/tactics/AML.TA0010) or for use in [ML Attack Staging](/tactics/AML.TA0001).
ML artifacts include models and datasets as well as other telemetry data produced when interacting with a model.
",AML.TA0009,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0035,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0009,"The adversary is trying to gather ML artifacts and other related information relevant to their goal.

Collection consists of techniques adversaries may use to gather information and the sources information is collected from that are relevant to following through on the adversary's objectives.
Frequently, the next goal after collecting data is to steal (exfiltrate) the ML artifacts, or use the collected information to stage future operations.
Common target sources include software repositories, container registries, model repositories, and object stores.
",AML.T0036,"Adversaries may leverage information repositories to mine valuable information.
Information repositories are tools that allow for storage of information, typically to facilitate collaboration or information sharing between users, and can store a wide variety of data that may aid adversaries in further objectives, or direct access to the target information.

Information stored in a repository may vary based on the specific instance or environment.
Specific common information repositories include Sharepoint, Confluence, and enterprise databases such as SQL Server.
",AML.TA0009,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0036,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0009,"The adversary is trying to gather ML artifacts and other related information relevant to their goal.

Collection consists of techniques adversaries may use to gather information and the sources information is collected from that are relevant to following through on the adversary's objectives.
Frequently, the next goal after collecting data is to steal (exfiltrate) the ML artifacts, or use the collected information to stage future operations.
Common target sources include software repositories, container registries, model repositories, and object stores.
",AML.T0036,"Adversaries may leverage information repositories to mine valuable information.
Information repositories are tools that allow for storage of information, typically to facilitate collaboration or information sharing between users, and can store a wide variety of data that may aid adversaries in further objectives, or direct access to the target information.

Information stored in a repository may vary based on the specific instance or environment.
Specific common information repositories include Sharepoint, Confluence, and enterprise databases such as SQL Server.
",AML.TA0009,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0035,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0005,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0043.003,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0042,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0043.001,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0042,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
",AML.T0043,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0043.003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0005.001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0043.002,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0005,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0005,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0043.000,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0043.002,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0043.000,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0043.001,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0005,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0043.000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0042,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0043.004,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0005,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0043.002,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0042,"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.
This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.
The adversary may verify the attack once but use it against many edge devices running copies of the target model.
The adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.
Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.
",AML.TA0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0042,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0005,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0043.003,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
",AML.T0042,"Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0043.001,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
",AML.T0042,"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
",AML.T0043,"An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
Interestingly enough, the compile time was the same for all the samples.
After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
The variants wouldn't always be executable but still classified as the same ransomware family.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
",AML.T0043.003,"Researchers at Skylight were able to create a universal bypass string that
when appended to a malicious file evades detection by Cylance's AI Malware detector.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0005.001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0043.002,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
",AML.T0005,"OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model.
Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0005,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0043.000,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
",AML.T0043.002,"CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0043.000,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0043.001,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0005,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0043.000,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0042,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0043.004,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0005,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0043.002,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0001,"An adversary is leveraging their knowledge of and access to the target system to tailor the attack.

ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.
Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.
Some of these techniques can be performed in an offline manor and are thus difficult to mitigate.
These techniques are often used to achieve the adversary's end goal.
",AML.T0043,"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.
Effects can range from misclassification, to missed detections, to maximising energy consumption.
Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.
For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.

Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).

The adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.
This allows the adversary to gain confidence their attack is effective ""live"" environment where their attack may be noticed.
They can then use the attack at a later time to accomplish their goals.
An adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).
",AML.TA0001,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
",AML.T0042,"Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and shown that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0045,"Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization.

Proprietary training data is costly to collect and annotate and may be a target for [Exfiltration](/tactics/AML.TA0010) and theft.

MLaaS providers charge for use of their API.
An adversary who has stolen a model via [Exfiltration](/tactics/AML.TA0010) or via [Extract ML Model](/techniques/AML.T0024.002) now has unlimited use of that service without paying the owner of the intellectual property.
",AML.TA0011,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
",AML.T0015,"This type of attack can break through the traditional live detection model
and cause the misuse of face recognition.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0045,"Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization.

Proprietary training data is costly to collect and annotate and may be a target for [Exfiltration](/tactics/AML.TA0010) and theft.

MLaaS providers charge for use of their API.
An adversary who has stolen a model via [Exfiltration](/tactics/AML.TA0010) or via [Extract ML Model](/techniques/AML.T0024.002) now has unlimited use of that service without paying the owner of the intellectual property.
",AML.TA0011,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0045,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0045,"Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization.

Proprietary training data is costly to collect and annotate and may be a target for [Exfiltration](/tactics/AML.TA0010) and theft.

MLaaS providers charge for use of their API.
An adversary who has stolen a model via [Exfiltration](/tactics/AML.TA0010) or via [Extract ML Model](/techniques/AML.T0024.002) now has unlimited use of that service without paying the owner of the intellectual property.
",AML.TA0011,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
",AML.T0015,"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0045,"Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization.

Proprietary training data is costly to collect and annotate and may be a target for [Exfiltration](/tactics/AML.TA0010) and theft.

MLaaS providers charge for use of their API.
An adversary who has stolen a model via [Exfiltration](/tactics/AML.TA0010) or via [Extract ML Model](/techniques/AML.T0024.002) now has unlimited use of that service without paying the owner of the intellectual property.
",AML.TA0011,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
",AML.T0031,"Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes.
Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0045,"Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization.

Proprietary training data is costly to collect and annotate and may be a target for [Exfiltration](/tactics/AML.TA0010) and theft.

MLaaS providers charge for use of their API.
An adversary who has stolen a model via [Exfiltration](/tactics/AML.TA0010) or via [Extract ML Model](/techniques/AML.T0024.002) now has unlimited use of that service without paying the owner of the intellectual property.
",AML.TA0011,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",AML.T0015,"The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples."
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0045,"Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization.

Proprietary training data is costly to collect and annotate and may be a target for [Exfiltration](/tactics/AML.TA0010) and theft.

MLaaS providers charge for use of their API.
An adversary who has stolen a model via [Exfiltration](/tactics/AML.TA0010) or via [Extract ML Model](/techniques/AML.T0024.002) now has unlimited use of that service without paying the owner of the intellectual property.
",AML.TA0011,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
",AML.T0015,"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0045,"Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization.

Proprietary training data is costly to collect and annotate and may be a target for [Exfiltration](/tactics/AML.TA0010) and theft.

MLaaS providers charge for use of their API.
An adversary who has stolen a model via [Exfiltration](/tactics/AML.TA0010) or via [Extract ML Model](/techniques/AML.T0024.002) now has unlimited use of that service without paying the owner of the intellectual property.
",AML.TA0011,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
",AML.T0015,"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
"
AML.TA0011,"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.
",AML.T0045,"Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization.

Proprietary training data is costly to collect and annotate and may be a target for [Exfiltration](/tactics/AML.TA0010) and theft.

MLaaS providers charge for use of their API.
An adversary who has stolen a model via [Exfiltration](/tactics/AML.TA0010) or via [Extract ML Model](/techniques/AML.T0024.002) now has unlimited use of that service without paying the owner of the intellectual property.
",AML.TA0011,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
",AML.T0015,"Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via ""neural payload injection.""
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play, and found 54 apps that were vulnerable to attack, including popular security and safety critical applications used for as cash recognition, parental control, face authentication, and financial services among others.
"
